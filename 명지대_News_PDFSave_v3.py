from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time, base64, os, random, re
from datetime import datetime
from rapidfuzz import fuzz  # âœ… ì¶”ê°€: ì œëª© ìœ ì‚¬ë„ ë¹„êµìš©

# ìœ ì‚¬ë„ë¥¼ ìœ„í•œ ê¸°ì‚¬ ì •ê·œí™”
import unicodedata

# ê¸°ì‚¬ ì œëª© ì •ê·œí™” í•¨ìˆ˜
def normalize_title(t):
    t = t.lower()
    t = unicodedata.normalize('NFKC', t)  # â€˜ â€™ â†’ ' ë¡œ í†µì¼
    t = re.sub(r"[^ê°€-í£a-z0-9\s]", "", t)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return t.strip()

# ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
def similarity(a, b):
    a_n, b_n = normalize_title(a), normalize_title(b)
    return (fuzz.partial_ratio(a_n, b_n) * 0.6 +
            fuzz.token_set_ratio(a_n, b_n) * 0.4)


# ====== ê¸°ë³¸ ì„¤ì • ======
query = "ëª…ì§€ëŒ€"
today = datetime.now().strftime("%Y%m%d")
save_dir = f"./NewsPDFs/{query}_ë„¤ì´ë²„_{today}/"

os.makedirs(save_dir, exist_ok=True)

# ì–¸ë¡ ì‚¬ ìš°ì„ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸
PRESS_ORDER = [
    "ì¡°ì„ ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "ë™ì•„ì¼ë³´", "í•œê²¨ë ˆ", "ë¨¸ë‹ˆíˆ¬ë°ì´", "ë‚´ì¼ì‹ ë¬¸",
    "ë‰´ì‹œìŠ¤", "ë² ë¦¬íƒ€ìŠ¤ì•ŒíŒŒ", "ë§¤ì¼ì¼ë³´", "ëŒ€í•™ì €ë„", "ë‰´ë°ì¼ë¦¬", "í•œêµ­ëŒ€í•™ì‹ ë¬¸", "ë¹„ìš˜ë“œí¬ìŠ¤íŠ¸"
]

# ====== ë¸Œë¼ìš°ì € ì„¤ì • ======
options = Options()
options.add_argument("--start-maximized")
options.add_experimental_option("detach", True)

driver = webdriver.Chrome(options=options)
wait = WebDriverWait(driver, 15)

url = f"https://search.naver.com/search.naver?ssc=tab.news.all&where=news&sm=tab_jum&query={query}"
driver.get(url)

# ì˜µì…˜ ë²„íŠ¼ í´ë¦­
opt_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".btn_option._search_option_open_btn")))
opt_btn.click()
print("âœ… ê²€ìƒ‰ ì˜µì…˜ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
time.sleep(2)

# '1ì£¼' ë²„íŠ¼ í´ë¦­
week_btn = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[@class="txt" and text()="1ì£¼"]')))
week_btn.click()
time.sleep(2)

# ìŠ¤í¬ë¡¤ (ë Œë”ë§ ìœ ë„)
driver.execute_script("window.scrollTo(0, document.body.scrollHeight / 3)")
time.sleep(2)

# ë‰´ìŠ¤ ë¸”ë¡ íƒìƒ‰
news_blocks = driver.find_elements(
    By.CSS_SELECTOR,
    "div.sds-comps-vertical-layout.sds-comps-full-layout[data-template-type='vertical']"
)

print("\nğŸ“° Scraping Programì„ ì‹¤í–‰í•˜ê² ìŠµë‹ˆë‹¤.\n")
print(f"ì´ {len(news_blocks)}ê°œì˜ ë‰´ìŠ¤ ë¸”ë¡ íƒìƒ‰ ì¤‘...\n")

visited = set()
articles = []   # âœ… ëª¨ë“  ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥
skip_count = 0
i = 0

# ====== ê¸°ì‚¬ ë°˜ë³µ ======
for idx, block in enumerate(news_blocks, 1):
    print(f"ğŸ§© [{idx}] ë‰´ìŠ¤ ë¸”ë¡ ì²˜ë¦¬ ì¤‘...")

    main_links = block.find_elements(By.CSS_SELECTOR, "a[href][data-heatmap-target='.tit']")
    related_links = block.find_elements(By.CSS_SELECTOR, "div.kKg41qrHvplVksYUiHBW a[href]")
    all_links = main_links + related_links

    for link in all_links:
        href = link.get_attribute("href")
        if href and href.startswith("http") and href not in visited:
            visited.add(href)
            try:
                driver.execute_script(f"window.open('{href}', '_blank');")
                driver.switch_to.window(driver.window_handles[-1])
                time.sleep(random.uniform(1, 2))

                raw_title = driver.title.strip()
                print("raw_title", raw_title)

                # ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
                press_match = re.search(
                    r"(?:\s*(?:-|::|:|ï¼|ï½œ|\||â€”|â€§)\s*)([ê°€-í£A-Za-z0-9&Â·\s]+?)\s*(?:(?:[:ï¼š]{2,}|-)?\s*)$",
                    raw_title
                )
                press = press_match.group(1).strip() if press_match else ""
                print("ì‹ ë¬¸ì‚¬", press)

                # ì œëª© ì •ë¦¬
                main_title = re.split(r"[-<|:ï¼ï½œâ€§]", raw_title)[0].strip()
                remove_words = ["ëŒ€í•™ë‰´ìŠ¤", "ëŒ€í•™ì†Œì‹", "ëŒ€í•™êµìœ¡", "ê¸°ì‚¬ë³¸ë¬¸", "ëŒ€í•™", "ë‰´ìŠ¤", "ë³´ë„ìë£Œ", "ê¸°íš", "êµìœ¡ë‰´ìŠ¤", "ì–¸ë¡ ë³´ë„", "ê³µê°ì–¸ë¡ "]
                for w in remove_words:
                    main_title = main_title.replace(w, "")
                    press = press.replace("ê³µê°ì–¸ë¡ ", "")
                    press = press.replace("Eë™ì•„", "ë™ì•„ì¼ë³´")
                if not press:
                    press = "ëŒ€í•™ì €ë„,ì í”„ë³¼ê°™ì€ êµ¬ì¡°ì—†ëŠ” ì‹ ë¬¸ì‚¬"
                main_title = main_title.strip(" _-Â·â€”â€“")

                # ëª…ì§€ëŒ€ í•„í„°ë§
                if "ëª…ì§€ëŒ€" not in main_title:
                    skip_count += 1
                    print(f"âš ï¸ '{main_title}' â†’ 'ëª…ì§€ëŒ€' ë¯¸í¬í•¨ (ëˆ„ë½ {skip_count}/3)")
                    driver.close()
                    driver.switch_to.window(driver.window_handles[0])
                    if skip_count >= 3:
                        print("\nğŸš¨ 'ëª…ì§€ëŒ€' ë¯¸í¬í•¨ ê¸°ì‚¬ 3íšŒ ì—°ì† â†’ ì¢…ë£Œ")
                        break
                    continue
                else:
                    skip_count = 0

                # PDF ì €ì¥ ëŒ€ì‹  ë©”ëª¨ë¦¬ì— ë‹´ê¸°
                pdf_data = driver.execute_cdp_cmd("Page.printToPDF", {
                    "printBackground": True,
                    "landscape": False,
                    "scale": 1
                })
                pdf_bytes = base64.b64decode(pdf_data["data"])

                articles.append({
                    "main_title": main_title,
                    "press": press,
                    "pdf_bytes": pdf_bytes
                })
                print(f"ğŸ—‚ ì„ì‹œ ì €ì¥: {main_title} ({press})")

                driver.close()
                driver.switch_to.window(driver.window_handles[0])
                time.sleep(random.uniform(2, 3))

            except Exception as e:
                print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
                driver.switch_to.window(driver.window_handles[0])

# ====== ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ í›„ ê·¸ë£¹í™” ======
print(f"\nâœ… ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ ({len(articles)}ê°œ)\n")
print("ğŸ“‚ ìœ ì‚¬ ì œëª© ê·¸ë£¹í™” ë° í´ë” ì •ë¦¬ ì¤‘...\n")

grouped = []
for art in articles:
    placed = False
    for g in grouped:
        if similarity(art["main_title"], g["rep"]) > 80:
            g["items"].append(art)
            placed = True
            break
    if not placed:
        grouped.append({"rep": art["main_title"], "items": [art]})

for g in grouped:
    folder_name = re.sub(r'[\\/*?:"<>|]', "_", g["rep"].strip())
    folder_path = os.path.join(save_dir, folder_name)
    os.makedirs(folder_path, exist_ok=True)

     # âœ… ì €ì¥ ìˆœì„œë¥¼ ì‹ ë¬¸ì‚¬ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    g["items"].sort(
        key=lambda x: PRESS_ORDER.index(x["press"]) if x["press"] in PRESS_ORDER else len(PRESS_ORDER)
    )

    for art in g["items"]:
        press_clean = re.sub(r'[\\/*?:"<>|]', "_", art["press"].strip())
        pdf_path = os.path.join(folder_path, f"{press_clean}.pdf")
        with open(pdf_path, "wb") as f:
            f.write(art["pdf_bytes"])
        print(f"âœ… {press_clean}.pdf - ({folder_name})")

print("\nğŸ‰ ëª¨ë“  ìœ ì‚¬ ê¸°ì‚¬ í´ë” ì •ë¦¬ ì™„ë£Œ!")
print(f"ğŸ“ ìµœì¢… ì €ì¥ ê²½ë¡œ: {os.path.abspath(save_dir)}")
print(f"\nâœ… ëª¨ë“  ê¸°ì‚¬ ë° ê´€ë ¨ê¸°ì‚¬ PDF ì €ì¥ ì™„ë£Œ! ({len(visited)}ê°œ ì €ì¥ë¨)")
